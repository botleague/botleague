Short few second test (thus sanity) of end-to-end driving performance with one camera in the following visual input modalities (i.e. view modes):

![Imgur](https://i.imgur.com/03QLmdk.jpg)

Included in these view modes were depth, world normals, reflectivity, and roughness which can be seen as an approximation to a Lidar sensor. All of these modes are part of Unreal's graphics pipelines making them efficient to render.

The hope is that this provides better generalization to real-world visual domains a la https://openai.com/blog/learning-dexterity/
